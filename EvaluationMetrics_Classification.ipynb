{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the target variable\n",
    "\n",
    "card_values = {\n",
    "    \"yes\": 1,\n",
    "    \"no\": 0\n",
    "}\n",
    "data[\"card\"] = data.card.map(card_values)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing numerical and categorical variables\n",
    "\n",
    "numerical = [\"reports\", \"age\", \"income\", \"share\", \"expenditure\", \"dependents\", \"months\", \"majorcards\", \"active\"]\n",
    "categorical = [\"owner\", \"selfemp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "\n",
    "full_train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "train, val = train_test_split(full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "y_train = train.card\n",
    "y_val = val.card\n",
    "y_test = test.card\n",
    "\n",
    "del train['card']\n",
    "del val['card']\n",
    "del test['card']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROC AUC score\n",
    "\n",
    "for c in numerical:\n",
    "    auc = roc_auc_score(y_train, train[c])\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(y_train, -train[c])\n",
    "    print('%9s, %.3f' % (c, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_train, train.share)\n",
    "plt.plot(fpr, tpr, label='-share')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "\n",
    "columns = categorical + numerical \n",
    "\n",
    "train_dicts = train[columns].to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "x_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "val_dicts = val[columns].to_dict(orient='reocrds')\n",
    "x_val = dv.transform(val_dicts)\n",
    "\n",
    "y_pred = model.predict_proba(x_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting ROC AUC Score\n",
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bin = model.predict(x_val)\n",
    "roc_auc_score(y_val, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the ROC Curve\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
    "plt.plot(fpr, tpr, label='probability') \n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred_bin)\n",
    "plt.plot(fpr, tpr, label='hard prediction')\n",
    "\n",
    "plt,plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "def confusion_matrix(y_val, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_val < t)\n",
    "\n",
    "        tp = (predict_positive & actual_negative).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        fp = (predict_positive & actual_positive).sum()\n",
    "        fn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    return df_scores\n",
    "\n",
    "df_scores = confusion_matrix(y_val, y_pred)\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating precision and recall\n",
    "df_scores['p'] = df_scores.tp / (df_scores.tp + df_scores.fp)\n",
    "df_scores['r'] = df_scores.tp / (df_scores.tp + df_scores.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a threshold\n",
    "plt.plot(df_scores.thresholds, df_scores.p, label='precision')\n",
    "plt.plot(df_scores.thresholds, df_scores.r, label='recall')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the F1 score\n",
    "plt.plot(df_scores.thresholds, df_scores.f1)\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train and prediction functions\n",
    "\n",
    "def train(train, y_train, C=1.0):\n",
    "    dicts = train[columns].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    x_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "\n",
    "    dicts = df[columns].to_dict(orient='reocrds')\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(x)[:, 1]\n",
    "\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing k-fold cross validation\n",
    "\n",
    "scores = []\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10]:\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(full_train):\n",
    "        train = full_train.iloc[train_idx]\n",
    "        val = full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = train.card\n",
    "        y_val = val.card\n",
    "\n",
    "        dv, model = train(train, y_train, C=C)\n",
    "        y_pred = predict(val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%4s, %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bcef26ca98546d29492bb7ccb9c24df4b97006e26fb72842bfe25a2d13649a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
